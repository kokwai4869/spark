{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Is the stream ready?\n",
      "True\n",
      " \n",
      "Schema of the input Stream\n",
      "<bound method DataFrame.printSchema of DataFrame[category: string, on twitter since: string, twitter_handle: string, profile url: string, followers: string, following: string, profile_location: string, profile lat/lon: string, profile description: string]>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1f8cbcd122dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'console'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'truncate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'false'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numRows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/test/lib/python3.7/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/test/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/test/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/test/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sparkSession = SparkSession.builder.master('local')\\\n",
    "                                    .appName('SparkStreamingAppendMode')\\\n",
    "                                    .getOrCreate()\n",
    "    \n",
    "    sparkSession.sparkContext.setLogLevel('ERROR')\n",
    "    \n",
    "    schema = StructType([StructField('category', StringType(), True),\n",
    "                         StructField('on twitter since', StringType(), True),\n",
    "                         StructField('twitter handle', StringType(), True),\n",
    "                         StructField('profile url', StringType(), True),\n",
    "                         StructField('followers', StringType(), True),\n",
    "                         StructField('following', StringType(), True),\n",
    "                         StructField('profile location', StringType(), True),\n",
    "                         StructField('profile lat/lon', StringType(), True),\n",
    "                         StructField('profile description', StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    fileStreamDF = sparkSession.readStream\\\n",
    "                                .option('header', 'true')\\\n",
    "                                .schema(schema)\\\n",
    "                                .csv('../input/datasets/dropfolder')\n",
    "    \n",
    "    fileStreamDF = fileStreamDF.withColumnRenamed('twitter handle', 'twitter_handle')\\\n",
    "                                .withColumnRenamed('profile location', 'profile_location')\n",
    "    \n",
    "    print(' ')\n",
    "    print('Is the stream ready?')\n",
    "    print(fileStreamDF.isStreaming)\n",
    "    \n",
    "    print(' ')\n",
    "    print('Schema of the input Stream')\n",
    "    print(fileStreamDF.printSchema)\n",
    "    \n",
    "    trimmedDF = fileStreamDF.select(fileStreamDF.category,\n",
    "                                   fileStreamDF.twitter_handle,\n",
    "                                   fileStreamDF.profile_location,\n",
    "                                   fileStreamDF.followers)\\\n",
    "                                    .withColumnRenamed('followers', 'companions')\n",
    "    \n",
    "    query = trimmedDF.writeStream\\\n",
    "                        .outputMode('append')\\\n",
    "                        .format('console')\\\n",
    "                        .option('truncate', 'false')\\\n",
    "                        .option('numRows', 30)\\\n",
    "                        .start()\\\n",
    "                        .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default count\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sparkSession = SparkSession.builder.master('local')\\\n",
    "                                    .appName('SparkStreamingCompleteMode')\\\n",
    "                                    .getOrCreate()\n",
    "    \n",
    "    sparkSession.sparkContext.setLogLevel('ERROR')\n",
    "    \n",
    "    schema = StructType([StructField('category', StringType(), True),\n",
    "                         StructField('on twitter since', StringType(), True),\n",
    "                         StructField('twitter handle', StringType(), True),\n",
    "                         StructField('profile url', StringType(), True),\n",
    "                         StructField('followers', StringType(), True),\n",
    "                         StructField('following', StringType(), True),\n",
    "                         StructField('profile location', StringType(), True),\n",
    "                         StructField('profile lat/lon', StringType(), True),\n",
    "                         StructField('profile description', StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    fileStreamDF = sparkSession.readStream\\\n",
    "                                .option('header', 'true')\\\n",
    "                                .option('maxFilesPerTrigger', 1)\\\n",
    "                                .schema(schema)\\\n",
    "                                .csv('../input/datasets/dropfolder')\n",
    "\n",
    "    # groupby category and find the count with default count\n",
    "    recordsPerCategory = fileStreamDF.groupby('category')\\\n",
    "                                        .count()\\\n",
    "                                        .orderBy('count', ascending=False)\n",
    "    \n",
    "    query = trimmedDF.writeStream\\\n",
    "                        .outputMode('complete')\\\n",
    "                        .format('console')\\\n",
    "                        .option('truncate', 'false')\\\n",
    "                        .option('numRows', 30)\\\n",
    "                        .start()\\\n",
    "                        .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate functions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sparkSession = SparkSession.builder.master('local')\\\n",
    "                                    .appName('SparkStreamingAggregate')\\\n",
    "                                    .getOrCreate()\n",
    "    \n",
    "    sparkSession.sparkContext.setLogLevel('ERROR')\n",
    "    \n",
    "    schema = StructType([StructField('category', StringType(), True),\n",
    "                         StructField('on twitter since', StringType(), True),\n",
    "                         StructField('twitter handle', StringType(), True),\n",
    "                         StructField('profile url', StringType(), True),\n",
    "                         StructField('followers', StringType(), True),\n",
    "                         StructField('following', StringType(), True),\n",
    "                         StructField('profile location', StringType(), True),\n",
    "                         StructField('profile lat/lon', StringType(), True),\n",
    "                         StructField('profile description', StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    fileStreamDF = sparkSession.readStream\\\n",
    "                                .option('header', 'true')\\\n",
    "                                .option('maxFilesPerTrigger', 1)\\\n",
    "                                .schema(schema)\\\n",
    "                                .csv('../input/datasets/dropfolder')\n",
    "\n",
    "    # groupby the dataframe based on category column and find the sum with aggregate functions\n",
    "    recordsPerCategory = fileStreamDF.groupby('category')\\\n",
    "                                        .agg({'followers': 'sum'})\\\n",
    "                                        .withColumnRenamed('sum(followers)', 'total_followers')\n",
    "                                        .orderBy('total_followers', ascending=False)\n",
    "    \n",
    "    query = trimmedDF.writeStream\\\n",
    "                        .outputMode('complete')\\\n",
    "                        .format('console')\\\n",
    "                        .option('truncate', 'false')\\\n",
    "                        .option('numRows', 30)\\\n",
    "                        .start()\\\n",
    "                        .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sql functions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sparkSession = SparkSession.builder.master('local')\\\n",
    "                                    .appName('SparkStreamingSQLQuery')\\\n",
    "                                    .getOrCreate()\n",
    "    \n",
    "    sparkSession.sparkContext.setLogLevel('ERROR')\n",
    "    \n",
    "    schema = StructType([StructField('category', StringType(), True),\n",
    "                         StructField('on twitter since', StringType(), True),\n",
    "                         StructField('twitter handle', StringType(), True),\n",
    "                         StructField('profile url', StringType(), True),\n",
    "                         StructField('followers', StringType(), True),\n",
    "                         StructField('following', StringType(), True),\n",
    "                         StructField('profile location', StringType(), True),\n",
    "                         StructField('profile lat/lon', StringType(), True),\n",
    "                         StructField('profile description', StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    fileStreamDF = sparkSession.readStream\\\n",
    "                                .option('header', 'true')\\\n",
    "                                .option('maxFilesPerTrigger', 1)\\\n",
    "                                .schema(schema)\\\n",
    "                                .csv('../input/datasets/dropfolder')\n",
    "    \n",
    "    fileStreamDF.createOrReplaceTempView('disaster_accident_crime_accounts')\n",
    "    \n",
    "    categoryDF = SparkSession.sql(\"SELECT from category, following\\\n",
    "                                    FROM disaster_accident_crime_accounts\\\n",
    "                                    WHERE followers > '15000'\")\n",
    "    \n",
    "    from pyspark.sql.functions import format_number\n",
    "    from pyspark.sql.functions import col\n",
    "    \n",
    "\n",
    "    # groupby the dataframe based on category column and find the sum with aggregate functions\n",
    "    recordsPerCategory = fileStreamDF.groupby('category')\\\n",
    "                                        .agg({'followers': 'sum'})\\\n",
    "                                        .withColumnRenamed('sum(followers)', 'total_followers')\n",
    "                                        .orderBy('total_followers', ascending=False)\\\n",
    "                                        .withColumn('total_following', format_number(col('total_following'), 0))\n",
    "    \n",
    "    query = trimmedDF.writeStream\\\n",
    "                        .outputMode('complete')\\\n",
    "                        .format('console')\\\n",
    "                        .option('truncate', 'false')\\\n",
    "                        .option('numRows', 30)\\\n",
    "                        .start()\\\n",
    "                        .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sparkSession = SparkSession.builder.master('local')\\\n",
    "                                    .appName('SparkStreamingAddTimeStamp')\\\n",
    "                                    .getOrCreate()\n",
    "    \n",
    "    sparkSession.sparkContext.setLogLevel('ERROR')\n",
    "    \n",
    "    schema = StructType([StructField('category', StringType(), True),\n",
    "                         StructField('on twitter since', StringType(), True),\n",
    "                         StructField('twitter handle', StringType(), True),\n",
    "                         StructField('profile url', StringType(), True),\n",
    "                         StructField('followers', StringType(), True),\n",
    "                         StructField('following', StringType(), True),\n",
    "                         StructField('profile location', StringType(), True),\n",
    "                         StructField('profile lat/lon', StringType(), True),\n",
    "                         StructField('profile description', StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    fileStreamDF = sparkSession.readStream\\\n",
    "                                .option('header', 'true')\\\n",
    "                                .option('maxFilesPerTrigger', 1)\\\n",
    "                                .schema(schema)\\\n",
    "                                .csv('../input/datasets/dropfolder')\n",
    "    \n",
    "    fileStreamDF = fileStreamDF.withColumnRenamed('twitter handle', 'twitter_handle')\\\n",
    "                            .withColumnRenamed('profile location', 'profile_location')\n",
    "    \n",
    "    def add_timestamp():\n",
    "        ts = time.time()\n",
    "        timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return timestamp\n",
    "    \n",
    "    add_timestamp_udf = udf(add_timestamp, StringType())\n",
    "    \n",
    "    fileStreamWithTS = fileStreamDF.withColumn('timestamp', add_timestamp_udf())\n",
    "    \n",
    "    trimmedDF = fileStreamWithTS.select('category', \n",
    "                                       'twitter_handle', \n",
    "                                       'followers',\n",
    "                                       'timestamp')\n",
    "    \n",
    "    query = trimmedDF.writeStream\\\n",
    "                        .outputMode('append')\\\n",
    "                        .format('console')\\\n",
    "                        .option('truncate', 'false')\\\n",
    "                        .option('numRows', 30)\\\n",
    "                        .start()\\\n",
    "                        .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
